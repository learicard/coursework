{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "\n",
    "IFT6135 â€“ Representation Learning\n",
    "\n",
    "A Deep Learning Course, January 2018\n",
    "\n",
    "By Chin-Wei Huang \n",
    "\n",
    "(Adapted from Sandeep Subramanian's 2017 MILA tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your Own Modules\n",
    "\n",
    "### `torch.nn.module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``nn.Module`` is base class for all neural network modules.\n",
    "\n",
    "You should also write your modules as sub-class of ``nn.Module``, so that it can inherit the following attributes:\n",
    "\n",
    "* *Recursive structure*: you can wrap an instantiation of a Module class with another one, which stores the inner one as its parent\n",
    "\n",
    "* cudafiability: you can easily cudafy the whole sequence of modules using `model.cuda()`\n",
    "\n",
    "* *Serializable*: you can save your trained model (checkpoint, early stopping ...) using ``torch.save``, ``torch.load``\n",
    "\n",
    "etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/docs/master/nn.html#linear-layers\n",
    "class Linear(nn.Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, in\\_features)`\n",
    "        - Output: :math:`(N, out\\_features)`\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape (out_features x in_features)\n",
    "        bias:   the learnable bias of the module of shape (out_features)\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            return self._backend.Linear()(input, self.weight)\n",
    "        else:\n",
    "            return self._backend.Linear()(input, self.weight, self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            return torch.mm(input, self.weight) \n",
    "        else:\n",
    "            return torch.mm(input, self.weight) + self.bias\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.from_numpy(np.random.randn(2, 3))).float()\n",
    "\n",
    "\n",
    "linear1 = nn.Linear(3,4)\n",
    "linear2 = MyLinear(3,4)\n",
    "\n",
    "# set the weight and bias of linear2 to be the same as linear1's\n",
    "linear2.weight.data = linear1.weight.data.transpose(1,0)\n",
    "linear2.bias.data = linear1.bias.data\n",
    "\n",
    "print(torch.eq(linear1(x), linear2(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet example\n",
    "\n",
    "* Resnet blocks let the gradient flow through the hidden unit more directly and at the same time increase expressiveness\n",
    "\n",
    "Res(x) = F(x, {W}) + x\n",
    "\n",
    "Res(x) = F(x, {W1}) + W2 x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if in_features != out_features:\n",
    "            self.project_linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inner = self.activation(self.linear(x))\n",
    "        if self.in_features != self.out_features:\n",
    "            skip = self.project_linear(x)\n",
    "        else:\n",
    "            skip = x\n",
    "        return inner + skip\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.from_numpy(np.random.randn(2, 3))).float()\n",
    "\n",
    "\n",
    "res1 = nn.Linear(3,3)\n",
    "res2 = MyLinear(3,5)\n",
    "\n",
    "print(res1(x).size())\n",
    "print(res2(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting things altogether, Sequential, Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.predict_ = nn.Sequential(\n",
    "            ResLinear(784, 328),\n",
    "            nn.ReLU(),\n",
    "            ResLinear(328, 328),\n",
    "            nn.ReLU(),\n",
    "            ResLinear(328, 10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return nn.Softmax()(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return torch.max(self.predict_proba(x))[1]\n",
    "    \n",
    "    def loss(self, x, target):\n",
    "        proba = self.predict_(x)\n",
    "        return self.criterion(proba, target)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caveate:    ``CrossEntropyLoss``    versus    ``NLLLoss``\n",
    "\n",
    "\n",
    "* ``CrossEntropyLoss`` takes in *pre-softmax* as input\n",
    "\n",
    "* ``NLLLoss`` takes in *log-softmax* as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.8393\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.8393\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = Variable(torch.Tensor(1,10).normal_())\n",
    "t = Variable(torch.from_numpy(np.random.choice(10, size=1)))\n",
    "\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "loss2 = nn.NLLLoss()\n",
    "\n",
    "print(loss1(y, t))\n",
    "print(loss2(nn.LogSoftmax(dim=1)(y), t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.4260\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.from_numpy(np.random.randn(64, 784))).float()\n",
    "t = Variable(torch.from_numpy(np.random.choice(10, size=64)))\n",
    "\n",
    "model = MyModel()\n",
    "print(model.loss(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters (Manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.4742\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.5233\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1643\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7926\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.4097\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2453\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1749\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1344\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1071\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.7845\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.from_numpy(np.random.randn(64, 784))).float()\n",
    "t = Variable(torch.from_numpy(np.random.choice(10, size=64)))\n",
    "model = MyModel()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    loss = model.loss(x, t)\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        #param.data = param.data - lr*param.grad.data\n",
    "        param.data.sub_(param.grad.data*lr)\n",
    "        param.grad.data.zero_()\n",
    "        \n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters (``torch.optim``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.4319\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.6166\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2979\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7428\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.4813\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.3368\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2404\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1464\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1203\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.from_numpy(np.random.randn(64, 784))).float()\n",
    "t = Variable(torch.from_numpy(np.random.choice(10, size=64)))\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.0)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.loss(x, t)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
