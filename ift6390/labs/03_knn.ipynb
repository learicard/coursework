{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little theory on kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm $k$ Nearest Neighbours is one of the simpler machine learning algorithms. It is motivated by the ideas that similar examples $x_t$ should have similar targets $y_t$. So, to define an algorithm $k$-NN you only need to define what you mean by *similar* in the context of examples and define how neighbours influence the prediction of a target for test examples.\n",
    "\n",
    "So, to predict the target class of a test examples $x$, all we need to do is find the $k$ nearest neighbours to $x$ using some metric (for example, euclidean distance also known as $L_2$ norm, or more generally minkowski distance $L_p$). Then we use those $k$ nearest neighbours to predict the target class of $x$. In a classification task, we would predict the target of $x$ to be the most common target of it's neighbours i.e. it's as though each neighbour of $x$ casts a vote for their own target class and the class with the most votes wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematic Formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "* $x$ be a test examples\n",
    "* $m$ be the number of classes\n",
    "* $D_n = \\{(x_t,y_t)\\}_{t=1}^n$ be the training data where $y_t \\in Y=\\{1,\\dots,m\\}$ is the corresponding target class of example $x_t$\n",
    "* $d(\\dot{},\\dot{})$ be our distance metric\n",
    "* $V(x,T,d(\\dot{},\\dot{}),k)$ l'ensemble des $k$ plus proches voisins de $x$ parmi les entrées de $T$ ainsi que leur cible associée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction by the $k$-NN algorithm is therefore\n",
    "\n",
    "> $$f(x)={\\mbox{arg max}} \\left(\\frac{1}{k} \\sum_{(x_i,y_i) \\in V(x)} \\mathrm{onehot}_{m}(y_i)\\right)$$\n",
    "\n",
    "A common distance function is the euclidean distance function\n",
    "> $$d(a,b)= \\sqrt{\\sum_{i=1}^d(a_i-b_i)^2}$$\n",
    "\n",
    "which is a specific case of the $L_p$ norm of Minkowski (where $p = 2$)\n",
    "> $$d(a,b)= \\left(\\sum_{i=1}^d|a_i-b_i|^p\\right)^\\frac{1}{p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a machine learning algorithm by specifying its' training procedure for some training data and how to predict the target for a test example. Given that the training procedure for $k$-NN is simply loading the training data $D_n$, we can specify how to predict the target class for the case when $k = 1$:\n",
    "\n",
    "    def 1-kNN(x)\n",
    "        min = +inf # intialize the distance of the nearest neighbour\n",
    "        idx = -1 # initialize the index of the nearest neighbour\n",
    "        \n",
    "        for t=1 to n\n",
    "            dt = d(X[t], x)\n",
    "            if dt < min\n",
    "                min = dt\n",
    "                idx = t\n",
    "                \n",
    "        return Y[idx]\n",
    "\n",
    "This runs in $O(n(k+d))$ time but you can get $O(n(log(k)+d))$ time by using a priority queue (heap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it in practise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make a machine learning algorithm to identify flowers. We have three types of iris species and we will try to use the characteristics of each flower (features) to determine which species of iris it is (class). But you don't know anything about flowers! So we will learn this algorithm using a dataset of flower measurements and the classes those flowers correspond to (training data), and we will use 1-kNN! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the $L^p$ (Minkowski) distance between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a function that given two vectors (`np.array`) will output the Minkowski distance between them. Complete the function `minkowski_vec` below. Test it yourself on two vectors (you can import the iris dataset as we did in the tutorial to use real iris vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minkowski_vec(x1, x2, p=2.0):\n",
    "    dist = np.sum(np.abs(x1 - x2)**p)**(1.0/p)\n",
    "    return dist\n",
    "\n",
    "# for testing\n",
    "a = np.ones(5)\n",
    "b = np.zeros(5)\n",
    "print(minkowski_vec(a,b))\n",
    "print(minkowski_vec(a,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the [definition](http://en.wikipedia.org/wiki/Minkowski_distance) in case you need it\n",
    "\n",
    "**Note:** since this is a vector, we'll need to apply operations to the different dimensions. We could do this by iterating over the array one element at a time (e.g. here to calculate the difference in absolute values)\n",
    "\n",
    "    s = 0\n",
    "    for i in range(x1.shape[0]):\n",
    "        s = s + abs(x1[i] - x2[i])\n",
    "\n",
    "or we could use `numpy` intelligently to do the same thing\n",
    "\n",
    "    s = numpy.sum(numpy.abs(x1 - x2))\n",
    "\n",
    "the difference is that the second option is not just more compact and easy to read, it uses `numpy`'s library to calculate the sums and operations which is much much faster than Python because they are specialized math functions written in C++. \n",
    "\n",
    "in short, use numpy functions instead of for loops where possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate $L^p$ distance between a vector and a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut aussi une fonction qui va nous permettre de comparer une fleur avec tout un ensemble de fleurs, sur la base de leurs traits. On va maintenant modifier la fonction `minkowski` pour calculer une *distance* $L^p$ entre un vecteur et une matrice (c.a.d. une fonction qui va nous retourner un vecteur de distances $L^p$)\n",
    "\n",
    "We also need a function to compare one flower to a bunch of other flowers. We will modify the function `minkowski_vec` to calculate an $L_p$ distance between a single vector (or 1D `np.array`) and a matrix (or 2D `np.array`). This function should return an array of distances corresponding to the distance between the single flower $x$ and each other flower in the bunch represented as rows in $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.23606798 2.23606798 2.23606798 2.23606798 2.23606798 2.23606798\n",
      " 2.23606798 2.23606798 2.23606798 2.23606798]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def minkowski_mat(x, Y, p=2.0):\n",
    "    dist = np.sum(np.abs(x - Y)**p, axis=1)**(1.0/p) \n",
    "    return dist\n",
    "\n",
    "# for testing\n",
    "a = np.ones((10,5))\n",
    "b = np.zeros((10,5))\n",
    "print(minkowski_mat(a,b))\n",
    "print(minkowski_mat(a,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** just like `minkowski_vec`, there are two ways to do this\n",
    " \n",
    "1. *Simple and inefficient:* write a loop that calls `minkowski_vec(x,Y[i,:],p)` for `x` and every row in `Y` and put it into `dist`.\n",
    "2. *More complicated but more efficient:* using a `numpy` mechanism called *broadcasting* to apply functions between the 1D array `x` and 2D array `Y`. For examples: `x - Y` returns an array of `[x - Y[i,:] for i in range(Y.shape(0))]` So here's the solution for the difference in absolute values:\n",
    "\n",
    "        def minkowski_mat(x,Y,p=2.0):\n",
    "            diff = x - Y # diff will be 2D\n",
    "            absdiff = abs(diff) # absdiff will be 2D\n",
    "            powdiff = absdiff**p # powdiff will be 2D\n",
    "            s = numpy.sum(powdiff,axis=1) # calculating the sum over the axis gives us a 1D array\n",
    "            dist = s**(1.0/p) # dist will also be 1D\n",
    "            return dist\n",
    "\n",
    "    or even shorter\n",
    "\n",
    "        def minkowski_mat(x,Y,p=2.0):\n",
    "            return (numpy.sum((abs(x-Y))**p,axis=1))**(1.0/p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reiterating the very important point **the vast majority of vector-vector, vector-matrix, or matrix-matrix operations are going to be much more efficient in numpy than in python using a for loop**. \n",
    "\n",
    "You may have notices that the difference in the efficient example implementations of `minkowski_vec` and `minkowski_mat` is just the part: `axis=1`. This exercise is to understand why it is important (and necessary) to specify on which axis you are applying the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're nearly there! Finish the following function to predict the species of iris given its' features `x` and verify its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x, data, p=2): \n",
    "    \n",
    "    feats = data[:, 0]\n",
    "    dist = minkowski_mat(x, feats, p)\n",
    "    \n",
    "    # find the index of the example with the minimum distance\n",
    "    min_dist = np.min(dist)\n",
    "    min_idx = np.where(dist == min_dist)[0]\n",
    "    \n",
    "    # return the target class of the example with that index\n",
    "    target_class = feats[min_idx, 1]\n",
    "    \n",
    "    return(target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `x` is the array of features (no target) of the test example. This functions should be quite easy to write then because `minkowski_mat` will output an array of distances, and we want to find the *index* of the *minimum* of those distances, then just find the target at that index.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now have all the components of the 1-NN algorithm, all that's left is to put it all together and test it\n",
    "\n",
    ">Remember that we can access the functions we've executed in the previous cells\n",
    "\n",
    "after testing your implementation, write a `for` loop which will call `knn(iris[i,:-1], iris, p)` for each example `i` and compare the prediction to the true target `iris[i,-1]`. The two should always be the same, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "iris = np.loadtxt('iris.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus and things to reflect on for the next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in machine learning we usually have a split of training data and testing data\n",
    "* divide the dataset in two : one training dataset with 100 examples (randomly sampled) and a testing dataset with the remaining examples. \n",
    "* Using the training data to find the nearest neighbours and target output for a given example, calculate the performance of your algorithm on training and testing. Why is there such a difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement a $k$-NN algorithm for a given $k > 1$\n",
    "* find the $k$ that optimizes performance on the training dataset\n",
    "* find the $k$ that optimizes performance on the testing dataset\n",
    "* explain the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* explain what happens if you set $k=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
