%&pdflatex
%% filename: amsart-template.tex, version: 2.1
\documentclass{amsart}
\usepackage{hyperref}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}
\setlength{\parindent}{0pt} % turn off auto-indent

\graphicspath{ {./} }

\begin{document}

\title{Assignment 1: [IFT6390]}

\author{L\'ea Ricard \& Joseph D. Viviano}
\address{Universit\'e de Montr\'eal}
\curraddr{}
\email{joseph@viviano.ca}
\thanks{}
\date{Sept 2018}

\maketitle

\section{Probabilities}

$P(D)$ = the probability of having disease. \\
$P(T)$ = the probability of having a positive test result. \\

\begin{itemize}
    \item 1.5\% of women in their 40s have breast cancer, therefore $P(D) = 0.015$.
    \item 87\% true positive rate, therefore $P(T|D) = 0.87$.
    \item 9.6\% false positive rate, therefore $P(T|\neg D) = 0.096$.
\end{itemize}

We want to know: \\

\begin{equation}
P(D|T) = \frac{P(T|D) P(D)}{P(T)}
\end{equation}

Which means we need to calculate $P(T)$ which is \\

\begin{equation}
   P(T) = P(T|D)P(D) + P(T |\neg D)P(\neg D) 
        = 0.87 \times 0.015 + 0.096 \times (1-0.015) 
        \approx 0.1076 
\end{equation}

\begin{equation}
    P(D|T) = \frac{0.87 \times 0.015}{0.1076} \approx 0.1213
\end{equation}

\section{Curse of Dimensionality}

(A) \textit{Consider a hypercube in dimension $d$ with side length $c$. 
            What is the volume $V$?} \\

In the 2-dimensional case, $area = c^2$. In the 3 dimensional case, $V = c^3$. 
In the n-dimensional case, $V = c^d$. \\

(B) \textit{$X$ is a random vector of dimension d ($x \in \R d$) distributed 
uniformly within the hypercube (the probability density  $p(x) = 0$ for all $x$ 
outside the cube). What is the probability density function $p(x)$ for $x$ inside 
the cube? Indicate which property(ies) of probability densities functions allow 
you to calculate this result.}\\

For all probability distributions: \\

\begin{equation}
    \int_{-\inf}^{\inf}p(x) = 1
\end{equation}

We know $p(x) = 0$ for all points outside of the hypercube. Therefore,
the probability of being in a particular point inside the hypercube is

\begin{equation}
    p(x) = \frac{1}{c^d}
\end{equation}

(C) \textit{Consider the outer shell (border) of the hypercube of width 
3\% of c (covering the part of the hypercube extending from the faces of 
the cube and 0.03c inwards). For example, if c = 100cm, the border will
be 3cm (left, right, top, etc ...) and will delimit this way a second
(inner) hypercube of side 100 − 3 − 3 = 94cm. If we generate a point $x$
according to the previously defined probability distribution (by 
sampling), what is the probability that it falls in the border area? What
is the probability that it falls in the smaller hypercube?}

Let $b$ be the amount to remove from the border on one side (i.e., left) 
of the outer hypercube.

$p(x_{large}) = 1$. Therefore in the general case the probability we are in the smaller hypercube is:\\

\begin{equation}
    p(x_{small}) = (c-2b)^d / c^d
\end{equation}

\begin{equation}
    p(x_{border}) = 1 - p(x_{small})
\end{equation}

Therefore for the above example: \\

\begin{equation}
    p(x_{small}) = (100 - 2 \times 3)^d / 100^d = 94^d / 100^d \\
\end{equation}

And as before: \\

\begin{equation}
    p(x_{border}) = 1 - p(x_{small})
\end{equation} \\

And the probability we are in the border is: \\

(D) \textit{Calculate the above for $d = {1, 2, 3, 5, 10, 100, 1000}$}.

\begin{equation}
    1- 94^1 / 100^1 = 0.06
\end{equation}

\begin{equation}
    94^2 / 100^2 = 0.1163
\end{equation}

\begin{equation}
    94^3 / 100^3 = 0.1694
\end{equation}

\begin{equation}
    94^5 / 100^5 = 0.2661
\end{equation}

\begin{equation}
    94^{10} / 100^{10} = 0.4614
\end{equation}

\begin{equation}
    94^{100} / 100^{100} = 0.9980
\end{equation}

\begin{equation}
    94^{1000} / 100^{1000} \approx 1
\end{equation} \\

(E) When the dimension grows, the probability that $x$ falls into the narrow 
border at the edge of the hypercube becomes more likely, which is contrary of 
our intuitions at lower dimensions. 

\section{Parametric Gaussian vs Parzen Window Density Estimation}

\subsection{Isotropic Gaussian Distribution} \\

(A) The named parameters are $\mu$, a $d$-long vector of means, and $\Sigma$, a 
$d \times d$ covariance matrix, where $n$ is the number of data points. \\

(B) 

\begin{equation}
    \mu = \frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}

\begin{equation}
    \Sigma = {1 \over {n}}\sum_{i=1}^n (\mathbf{x}_i-\mu) (\mathbf{x}_i-\mu)^\mathrm{T}
\end{equation} \\

(C) For the $\mathcal{\mu}$ parameter, the algorithm complexity is in $\mathcal{O}(n)$, since it is summing over the vectors $\mathcal (x_{i})$.

For the  $\Sigma$ parameter, since the Gaussian density function is isotropic, the algorithm complexity is also in $\mathcal{O}(n)$. 

%$\mu$ can be calculated by $\Sigma = \sigma I^2$. \\

(D)

\begin{equation}
    p(x) = \frac{1}{(2\pi)^{frac{d}{2} \sqrt{|\Sigma|}}} {\rm e}^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)}   
\end{equation}
 
(E) The cost of this operation is limited by the find the determinant of \Sigma
and theinversion of \Sigma. Both of these operations take $\mathcal{O}(d^{2.373})$  
assuming we use the Coppersmith–Winograd algorithm for the inversion and fast
matrix multiplication for finding the determinant. \\

\subsection{Parzen windows with Isotropic Gaussian Kernels} \\

(A) If $\sigma$ is fixed by the user, nothing is learned during training (the 
Gaussians are simply centered on each training data point and then summed to 
create a density). Basicially this algorithm learn to remember the data. \\

(B) \\

\begin{equation}
    p(x) = \frac{1}{n}\sum_{i=1}^{n}\mathcal{N}_{x, \sigma}(x)
\end{equation}

Expanded becomes: \\

\begin{equation}
    p(x) = \frac{1}{n}\sum_{i=1}^{n} \frac{1}{(2\pi)^{\frac{d}{2}} \sigma^d} {\rm e}^{-\frac{1}{2}\frac{d(x_{test}, x_{train})^2}{\sigma^2} }
\end{equation}\\

(C) $\mathcal{O}(n)$, since we have to calculate the distance between 
$x_{test}$ and all of the $n$ data points $x_{train}$. \\

\subsection{Capacity/Expressivitiy} \\

(A) The Parzen Gaussian is more expressive, because it can store information for
 every data point. The capacity of the algorithm grows as we give it more data 
 points, this isn't true for the Gaussian distribution, which averages over all 
 data points, so it has a fixed capacity for a given dimensionality, no matter 
 how many training data the algorithm is shown. \\

(B) Parzen windows with Isotropic Gaussian Kernels, in the case that we used a 
large number of training examples with a small $\sigma$ would result in extreme 
memorization of the noise in the training data (i.e., overfitting). \\

(C) Because in parametric Gaussian density estimation, $\sigma$ is learned from 
the data, while it is fixed for all data points when using Parzen windows. \\ 

\subsection{Empirical Risk}

(A) \textit{Express the equation of a diagonal Gaussian density in R^d . Specify 
            what are its parameters and their dimensions.} \\

p(x) = \frac{1}{(2\pi)^{frac{d}{2} \sqrt{|\Sigma|}}} {\rm e}^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)} \\

-- parameters are \Sigma (\d \time \d) and \mu (\d).

(B) \textit{Show that the components of a random vector following a diagonal 
            Gaussian distribution are independent random variables.} \\



(C) \textit{Using − log p(x) as the loss, write down the equation corresponding 
            to the empirical risk minimization on the training set D (in order 
            to learn the parameters).} \\

The log-likelyhood function is: \\

\begin{equation}
    log(P|\mu,\Sigma) = -\frac{N}{2}log|\Sigma| -\frac{1}{2} 
        \sum_{n=1}^{N} [(X_n - \mu)^T \Sigma^{-1} (X_n - \mu)] + c
\end{equation}

Where $c$ is a constant.

(D) \textit{Solve this equation analytically in order to obtain the optimal
            parameters.} \\


First we find \hat{\mu}: \\


\begin{equation}
    \frac{\partial log p(X | \mu, \Sigma)}{\partial\mu} = 
        \sum_{i=1}^{N}(X_i-\mu)^T \Sigma^-1 = 0 
\end{equation}


\begin{equation}
    0 = N \mu - \sum_{i=1}^{} 
\end{equation}

\begin{equat}


\section{Softmax Activation Function}

\end{document}

